{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment-analysis-dl.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPqX19SPdK1XsPYyi231ieA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faizuddin/ISB46703/blob/main/sentiment_analysis_dl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principles of AI (ISB46703)\n",
        "## Communication and Perception: Deep Learning for Natural Language Processing\n",
        "In this tutorial, we will look into how to implement a Sentiment Analysis (quite often referred simply as NLP) pipeline that combines a traditional supervised learning algorithm with a deep learning algorithm to interpret and classify emotions in text information.\n",
        "\n",
        "Hence, the main objective is going to be to demonstrate how to set up that pipeline that facilitates collection, preprocessing and classifying labeled text data to finally training and evaluating deep learning models in [Keras](https://keras.io). \n",
        "\n",
        "We also will look into overfit model, a case where the model memorises the noise and fits too closely to the training set, the model becomes *overfitted* and it is unable to generalise well to new data. If a model cannot generalise well to new data, then it will not be able to perform the classification or prediction tasks that it was intended for.\n",
        "\n",
        "### NLP Motivation\n",
        "Imagine the task of determining whether a product’s review is positive or negative; you could do it yourself just by reading it, right? But what happens when the company you work for sells 2000 products every single day? Would you read all the reviews and manually classify them? There’s where Sentiment Analysis comes in and makes your life and job easier.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4zMTdNaEqLwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "We start by importing the necessary packages and configuring some parameters. We will use Keras to fit the deep learning models. The training data is the [Twitter US Airline Sentiment data set from Kaggle](https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment)."
      ],
      "metadata": {
        "id": "4LLPyUe7rvon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "mtNIid0z3l0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfhA1yAQqBM9"
      },
      "outputs": [],
      "source": [
        "# Basic packages\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import re\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import nltk\n",
        "\n",
        "# Packages for data preparation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Packages for modeling\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import regularizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\n",
        "NB_START_EPOCHS = 20  # Number of epochs we usually start to train with\n",
        "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\n",
        "MAX_LEN = 20  # Maximum number of words in a sequence"
      ],
      "metadata": {
        "id": "jBir0CKnviUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "VIzospD_3xmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Enabling and testing the GPU\n",
        "\n",
        "You'll need to enable GPUs for the notebook to accelerate computation:\n",
        "Navigate to Edit→Notebook Settings\n",
        "Select GPU from the Hardware Accelerator drop-down.\n",
        "\n",
        "Next, confirm that we can connect to the GPU with tensorflow:"
      ],
      "metadata": {
        "id": "6-vnnxgt1QE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "J6Z3nEvW1XH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions\n",
        "We will use some helper functions throughout (to ensure our code is neat and readable!)"
      ],
      "metadata": {
        "id": "zI4kV5pivnIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deep_model(model, X_train, y_train, X_valid, y_valid):\n",
        "    '''\n",
        "    Function to train a multi-class model. The number of epochs and \n",
        "    batch_size are set by the constants at the top of the\n",
        "    notebook. \n",
        "    \n",
        "    Parameters:\n",
        "        model : model with the chosen architecture\n",
        "        X_train : training features\n",
        "        y_train : training target\n",
        "        X_valid : validation features\n",
        "        Y_valid : validation target\n",
        "    Output:\n",
        "        model training history\n",
        "    '''\n",
        "    model.compile(optimizer='rmsprop'\n",
        "                  , loss='categorical_crossentropy'\n",
        "                  , metrics=['accuracy'])\n",
        "    \n",
        "    history = model.fit(X_train\n",
        "                       , y_train\n",
        "                       , epochs=NB_START_EPOCHS\n",
        "                       , batch_size=BATCH_SIZE\n",
        "                       , validation_data=(X_valid, y_valid)\n",
        "                       , verbose=0)\n",
        "    return history\n",
        "\n",
        "\n",
        "def eval_metric(model, history, metric_name):\n",
        "    '''\n",
        "    Function to evaluate a trained model on a chosen metric. \n",
        "    Training and validation metric are plotted in a\n",
        "    line chart for each epoch.\n",
        "    \n",
        "    Parameters:\n",
        "        history : model training history\n",
        "        metric_name : loss or accuracy\n",
        "    Output:\n",
        "        line chart with epochs of x-axis and metric on\n",
        "        y-axis\n",
        "    '''\n",
        "    metric = history.history[metric_name]\n",
        "    val_metric = history.history['val_' + metric_name]\n",
        "\n",
        "    e = range(1, NB_START_EPOCHS + 1)\n",
        "\n",
        "    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
        "    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n",
        "    plt.xlabel('Epoch number')\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.title('Comparing training and validation ' + metric_name + ' for ' + model.name)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def test_model(model, X_train, y_train, X_test, y_test, epoch_stop):\n",
        "    '''\n",
        "    Function to test the model on new data after training it\n",
        "    on the full training data with the optimal number of epochs.\n",
        "    \n",
        "    Parameters:\n",
        "        model : trained model\n",
        "        X_train : training features\n",
        "        y_train : training target\n",
        "        X_test : test features\n",
        "        y_test : test target\n",
        "        epochs : optimal number of epochs\n",
        "    Output:\n",
        "        test accuracy and test loss\n",
        "    '''\n",
        "    model.fit(X_train\n",
        "              , y_train\n",
        "              , epochs=epoch_stop\n",
        "              , batch_size=BATCH_SIZE\n",
        "              , verbose=0)\n",
        "    results = model.evaluate(X_test, y_test)\n",
        "    print()\n",
        "    print('Test accuracy: {0:.2f}%'.format(results[1]*100))\n",
        "    return results\n",
        "\n",
        "    \n",
        "def remove_stopwords(input_text):\n",
        "    '''\n",
        "    Function to remove English stopwords from a Pandas Series.\n",
        "    \n",
        "    Parameters:\n",
        "        input_text : text to clean\n",
        "    Output:\n",
        "        cleaned Pandas Series \n",
        "    '''\n",
        "    stopwords_list = stopwords.words('english')\n",
        "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
        "    whitelist = [\"n't\", \"not\", \"no\"]\n",
        "    words = input_text.split() \n",
        "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
        "    return \" \".join(clean_words) \n",
        "    \n",
        "def remove_mentions(input_text):\n",
        "    '''\n",
        "    Function to remove mentions, preceded by @, in a Pandas Series\n",
        "    \n",
        "    Parameters:\n",
        "        input_text : text to clean\n",
        "    Output:\n",
        "        cleaned Pandas Series \n",
        "    '''\n",
        "    return re.sub(r'@\\w+', '', input_text)\n",
        "\n",
        "\n",
        "def compare_models_by_metric(model_1, model_2, model_hist_1, model_hist_2, metric):\n",
        "    '''\n",
        "    Function to compare a metric between two models \n",
        "    \n",
        "    Parameters:\n",
        "        model_hist_1 : training history of model 1\n",
        "        model_hist_2 : training history of model 2\n",
        "        metrix : metric to compare, loss, acc, val_loss or val_acc\n",
        "        \n",
        "    Output:\n",
        "        plot of metrics of both models\n",
        "    '''\n",
        "    metric_model_1 = model_hist_1.history[metric]\n",
        "    metric_model_2 = model_hist_2.history[metric]\n",
        "\n",
        "    e = range(1, NB_START_EPOCHS + 1)\n",
        "    \n",
        "    metrics_dict = {\n",
        "        'acc' : 'Training Accuracy',\n",
        "        'loss' : 'Training Loss',\n",
        "        'val_acc' : 'Validation accuracy',\n",
        "        'val_loss' : 'Validation loss'\n",
        "    }\n",
        "    \n",
        "    metric_label = metrics_dict[metric]\n",
        "\n",
        "    plt.plot(e, metric_model_1, 'bo', label=model_1.name)\n",
        "    plt.plot(e, metric_model_2, 'b', label=model_2.name)\n",
        "    plt.xlabel('Epoch number')\n",
        "    plt.ylabel(metric_label)\n",
        "    plt.title('Comparing ' + metric_label + ' between models')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "def optimal_epoch(model_hist):\n",
        "    '''\n",
        "    Function to return the epoch number where the validation loss is\n",
        "    at its minimum\n",
        "    \n",
        "    Parameters:\n",
        "        model_hist : training history of model\n",
        "\n",
        "    Output:\n",
        "        epoch number with minimum validation loss\n",
        "    '''\n",
        "    min_epoch = np.argmin(model_hist.history['val_loss']) + 1\n",
        "    print(\"Minimum validation loss reached in epoch {}\".format(min_epoch))\n",
        "    return min_epoch"
      ],
      "metadata": {
        "id": "h0M2-D2SwArg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data preparation\n",
        "\n",
        "###Data cleaning\n",
        "\n",
        "We load the csv with the tweets and perform a random shuffle. It's a good practice to shuffle the data before splitting between a train and test set. That way the sentiment classes are equally distributed over the train and test sets. We'll only keep the ***text*** column as input and the `airline_sentiment` column as the target.\n",
        "\n",
        "The next thing we'll do is ***removing stopwords***. Stopwords do not have any value for predicting the sentiment. Furthermore, as we want to build a model that can be used for other airline companies as well, we remove the ***mentions***. (i.e. @abcde)"
      ],
      "metadata": {
        "id": "P9HmdlSYwDUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Tweets.csv')\n",
        "df = df.reindex(np.random.permutation(df.index))  \n",
        "df = df[['text', 'airline_sentiment']]\n",
        "df.text = df.text.apply(remove_stopwords).apply(remove_mentions)"
      ],
      "metadata": {
        "id": "fAgA3dX9wdSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train-Test split\n",
        "\n",
        "The evaluation of the model performance needs to be done on a separate test set. As such, we can estimate how well the model generalises (this is important!). This is done with the `train_test_split` method of scikit-learn."
      ],
      "metadata": {
        "id": "HKmfrV8VweLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.text, df.airline_sentiment, test_size=0.1, random_state=37)\n",
        "print('# Train data samples:', X_train.shape[0])\n",
        "print('# Test data samples:', X_test.shape[0])\n",
        "assert X_train.shape[0] == y_train.shape[0]\n",
        "assert X_test.shape[0] == y_test.shape[0]"
      ],
      "metadata": {
        "id": "HAGmea9hwp-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Converting words to numbers\n",
        "\n",
        "To use the text as input for a model, we first need to convert the words into tokens, which simply means converting the words to integers that refer to an index in a dictionary. Here we will only keep the most frequent words in the training set.\n",
        "\n",
        "We clean up the text by applying ***filters*** and putting the words to ***lowercase***. Words are separated by spaces."
      ],
      "metadata": {
        "id": "rp49gjGfwtL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tk = Tokenizer(num_words=NB_WORDS,\n",
        "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "               lower=True,\n",
        "               char_level=False,\n",
        "               split=' ')\n",
        "tk.fit_on_texts(X_train)\n",
        "\n",
        "print('Fitted tokenizer on {} documents'.format(tk.document_count))\n",
        "print('{} words in dictionary'.format(tk.num_words))\n",
        "print('Top 5 most common words are:', collections.Counter(tk.word_counts).most_common(5))"
      ],
      "metadata": {
        "id": "kcDULQQZw4UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After having created the dictionary we can convert the text of a tweet to a vector with `NB_WORDS` values. With `mode=binary`, it contains an indicator whether the word appeared in the tweet or not. This is done with the `texts_to_matrix `method of the `Tokenizer`."
      ],
      "metadata": {
        "id": "dHW7iNIdw56-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_oh = tk.texts_to_matrix(X_train, mode='binary')\n",
        "X_test_oh = tk.texts_to_matrix(X_test, mode='binary')"
      ],
      "metadata": {
        "id": "O2MIsODexEGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Converting the target classes to numbers\n",
        "\n",
        "We need to convert the target classes to numbers as well, which in turn are one-hot-encoded with the `to_categorical` method in Keras"
      ],
      "metadata": {
        "id": "can8waunxEnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "y_train_le = le.fit_transform(y_train)\n",
        "y_test_le = le.transform(y_test)\n",
        "y_train_oh = to_categorical(y_train_le)\n",
        "y_test_oh = to_categorical(y_test_le)\n",
        "\n",
        "print('\"{}\" is converted into {}'.format(y_train[0], y_train_le[0]))\n",
        "print('\"{}\" is converted into {}'.format(y_train_le[0], y_train_oh[0]))"
      ],
      "metadata": {
        "id": "bTKHyQSCxMhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Splitting off a validation set\n",
        "\n",
        "Now that our data is ready, we split off a validation set. This validation set will be used to evaluate the model performance when we tune the parameters of the model."
      ],
      "metadata": {
        "id": "1i2d0w3OxNRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_rest, X_valid, y_train_rest, y_valid = train_test_split(X_train_oh, y_train_oh, test_size=0.1, random_state=37)\n",
        "\n",
        "assert X_valid.shape[0] == y_valid.shape[0]\n",
        "assert X_train_rest.shape[0] == y_train_rest.shape[0]\n",
        "\n",
        "print('Shape of validation set:',X_valid.shape)"
      ],
      "metadata": {
        "id": "23HmPmQ2xSO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Deep learning\n",
        "\n",
        "###Creating a model that overfits\n",
        "\n",
        "We start with a model that overfits. It has 2 densely connected layers of 64 elements. The `input_shape` for the first layer is equal to the number of words we kept in the dictionary and for which we created one-hot-encoded features.\n",
        "\n",
        "As we need to **predict 3 different sentiment classes**, the last layer has 3 elements. The ***softmax*** activation function makes sure the three probabilities sum up to 1.\n",
        "\n",
        "The number of parameters to train is computed as **(nb inputs x nb elements in hidden layer) + nb bias terms**. The number of inputs for the first layer equals the number of words in our corpus. The subsequent layers have the number of outputs of the previous layer as inputs. So the number of parameters per layer are:\n",
        "\n",
        "1. First layer : (10000 x 64) + 64 = 640064\n",
        "2. Second layer : (64 x 64) + 64 = 4160\n",
        "3. Last layer : (64 x 3) + 3 = 195"
      ],
      "metadata": {
        "id": "Cx8PtN2DxS25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = models.Sequential()\n",
        "base_model.add(layers.Dense(64, activation='relu', input_shape=(NB_WORDS,)))\n",
        "base_model.add(layers.Dense(64, activation='relu'))\n",
        "base_model.add(layers.Dense(3, activation='softmax'))\n",
        "base_model._name = \"baselineModel\"\n",
        "base_model.summary()"
      ],
      "metadata": {
        "id": "DlwnIOg6x5xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because this project is a **multi-class**, **single-label prediction**, we use `categorical_crossentropy` as the loss function and softmax as the final activation function. We fit the model on the train data and validate on the validation set. We run for a ***predetermined number of epochs*** and will see when the model starts to overfit."
      ],
      "metadata": {
        "id": "3mEuKbwdyPuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_history = deep_model(base_model, X_train_rest, y_train_rest, X_valid, y_valid)\n",
        "base_min = optimal_epoch(base_history)"
      ],
      "metadata": {
        "id": "tvW_IHitylk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_metric(base_model, base_history, 'loss')"
      ],
      "metadata": {
        "id": "O9hGMvKBypW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the beginning the validation loss goes down. But at epoch 3 this stops and the validation loss starts increasing rapidly. This is when the models begins to **overfit**.\n",
        "\n",
        "The training loss continues to go down and almost reaches zero at epoch 20. This is normal as the model is trained to fit the train data as good as possible."
      ],
      "metadata": {
        "id": "SDA7YE88yqX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Handling overfitting\n",
        "\n",
        "Now, we can try to do something about the overfitting. There are different options to do that.\n",
        "\n",
        "1. Option 1: **reduce the network's capacity** by removing layers or reducing the number of elements in the hidden layers\n",
        "2. Option 2: **apply regularization**, which comes down to adding a cost to the loss function for large weights\n",
        "3. Option 3: **use Dropout layers**, which will randomly remove certain features by setting them to zero\n",
        "\n",
        "###Reducing the network's capacity\n",
        "\n",
        "Our first model has a large number of trainable parameters. The higher this number, the easier the model can memorize the target class for each training sample. Obviously, this is not ideal for generalising on new data.\n",
        "\n",
        "By lowering the capacity of the network, you force it to learn the patterns that matter, or that minimize the loss. On the other hand, reducing the network's capacity too much, will lead to **underfitting**. The model will not be able to learn the relevant patterns in the train data.\n",
        "\n",
        "We reduce the network's capacity by removing one hidden layer and lowering the number of elements in the remaining layer to 16."
      ],
      "metadata": {
        "id": "AlR7w0aWy3EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_model = models.Sequential()\n",
        "reduced_model.add(layers.Dense(16, activation='relu', input_shape=(NB_WORDS,)))\n",
        "reduced_model.add(layers.Dense(3, activation='softmax'))\n",
        "reduced_model._name = 'reducedModel'\n",
        "reduced_model.summary()"
      ],
      "metadata": {
        "id": "Y_G4YTaozR-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_history = deep_model(reduced_model, X_train_rest, y_train_rest, X_valid, y_valid)\n",
        "reduced_min = optimal_epoch(reduced_history)"
      ],
      "metadata": {
        "id": "4KR5Ful5zUxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_metric(reduced_model, reduced_history, 'loss')"
      ],
      "metadata": {
        "id": "6b5WIzVpzX6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that it takes more epochs before the reduced model starts overfitting. The validation loss also goes up slower than our first model."
      ],
      "metadata": {
        "id": "Il0_2QMmzdNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_models_by_metric(base_model, reduced_model, base_history, reduced_history, 'val_loss')"
      ],
      "metadata": {
        "id": "r-Q4cJKEzl3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we compare the validation loss of the baseline model, it is clear that the reduced model starts overfitting at a later epoch. The validation loss stays lower much longer than the baseline model."
      ],
      "metadata": {
        "id": "eK1wheWxziGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Applying regularization\n",
        "\n",
        "To address overfitting, we can apply weight regularization to the model. This will add a cost to the loss function of the network for large weights (or parameter values). As a result, you get a simpler model that will be forced to learn only the relevant patterns in the train data.\n",
        "\n",
        "There are **L1** regularization and **L2** regularization.\n",
        "\n",
        "1. L1 regularization will add a cost with regards to the **absolute value of the parameters**. It will result in some of the weights to be equal to zero.\n",
        "2. L2 regularization will add a cost with regards to the **squared value of the parameters**. This results in smaller weights.\n",
        "\n",
        "Let's try with L2 regularization."
      ],
      "metadata": {
        "id": "krNTbUn1z1Ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reg_model = models.Sequential()\n",
        "reg_model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(NB_WORDS,)))\n",
        "reg_model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
        "reg_model.add(layers.Dense(3, activation='softmax'))\n",
        "reg_model._name = 'L1L2regModel'\n",
        "reg_model.summary()"
      ],
      "metadata": {
        "id": "3Jxr18FO0Gaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_history = deep_model(reg_model, X_train_rest, y_train_rest, X_valid, y_valid)\n",
        "reg_min = optimal_epoch(reg_history)"
      ],
      "metadata": {
        "id": "_y_A-u2J0JLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_metric(reg_model, reg_history, 'loss')"
      ],
      "metadata": {
        "id": "gJmGszvm0PwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the regularized model we notice that it starts overfitting in the same epoch as the baseline model. However, the loss increases much slower afterwards."
      ],
      "metadata": {
        "id": "JT_MNsf80NA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_models_by_metric(base_model, reg_model, base_history, reg_history, 'val_loss')"
      ],
      "metadata": {
        "id": "lwl-fWWG0SkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Adding dropout layers\n",
        "\n",
        "The last option we'll try is to add Dropout layers. A Dropout layer will randomly set output features of a layer to zero."
      ],
      "metadata": {
        "id": "dK_Fz3Gj0N_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drop_model = models.Sequential()\n",
        "drop_model.add(layers.Dense(64, activation='relu', input_shape=(NB_WORDS,)))\n",
        "drop_model.add(layers.Dropout(0.5))\n",
        "drop_model.add(layers.Dense(64, activation='relu'))\n",
        "drop_model.add(layers.Dropout(0.5))\n",
        "drop_model.add(layers.Dense(3, activation='softmax'))\n",
        "drop_model._name = 'dropoutModel'\n",
        "drop_model.summary()"
      ],
      "metadata": {
        "id": "61BhqUf_6j-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop_history = deep_model(drop_model, X_train_rest, y_train_rest, X_valid, y_valid)\n",
        "drop_min = optimal_epoch(drop_history)"
      ],
      "metadata": {
        "id": "lyQ4CUz10bIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_metric(drop_model, drop_history, 'loss')"
      ],
      "metadata": {
        "id": "IcXOLwDK0dAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model with dropout layers starts overfitting later than the baseline model. The loss also increases slower than the baseline model."
      ],
      "metadata": {
        "id": "wbwVmda70fk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_models_by_metric(base_model, drop_model, base_history, drop_history, 'val_loss')"
      ],
      "metadata": {
        "id": "-6Qd4VW80hkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model with the Dropout layers starts overfitting later. Compared to the baseline model the loss also remains much lower."
      ],
      "metadata": {
        "id": "M1zPJfL00kLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training on the full train data and evaluation on test data\n",
        "At first sight the reduced model seems to be the best model for generalisation. But let's check that on the test set."
      ],
      "metadata": {
        "id": "moEzGIBc0ni9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_results = test_model(base_model, X_train_oh, y_train_oh, X_test_oh, y_test_oh, base_min)"
      ],
      "metadata": {
        "id": "5DRtmphQ0to0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_results = test_model(reduced_model, X_train_oh, y_train_oh, X_test_oh, y_test_oh, reduced_min)"
      ],
      "metadata": {
        "id": "dKTOZud80vs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_results = test_model(reg_model, X_train_oh, y_train_oh, X_test_oh, y_test_oh, reg_min)"
      ],
      "metadata": {
        "id": "sJe-p4sg0yEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop_results = test_model(drop_model, X_train_oh, y_train_oh, X_test_oh, y_test_oh, drop_min)"
      ],
      "metadata": {
        "id": "Ln5bKyXA00p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusion\n",
        "\n",
        "We can identify overfitting by looking at validation metrics like loss or accuracy. Usually, the validation metric stops improving after a certain number of epochs and begins to decrease afterward. The training metric continues to improve because the model seeks to find the best fit for the training data.\n",
        "\n",
        "There are several manners in which we can reduce overfitting in deep learning models. The best option is to get more training data. Unfortunately, in real-world situations, you often do not have this possibility due to time, budget, or technical constraints.\n",
        "\n",
        "As shown above, all three options help to reduce overfitting. We manage to increase the accuracy on the test data substantially. Among these three options, the model with the **Reduced** model performs the best on the test data."
      ],
      "metadata": {
        "id": "NDdMrld704_-"
      }
    }
  ]
}